{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a004c9f5-5f9e-4faa-9137-d02699c75e06",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Knowledge Bases with S3 Vectors\n",
    "This notebook provides sample code for building an end-to-end example for building a RAG application using Amazon Bedrock Knowledge Bases using Amazon S3 Vector store as the vector database. This notebook contains:\n",
    "\n",
    "1. Overview\n",
    "2. Pre-requisites\n",
    "3. Creating an Amazon S3 Vector Store and Index\n",
    "4. Creating an Amazon Bedrock Knowledge Base\n",
    "5. Creating the Data Source\n",
    "6. Sync the Data Source\n",
    "7. Test the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127230f-e6fd-4ffc-afac-018cf0441ca7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Amazon Bedrock Knowledge Bases allows you to integrate proprietary information into your generative-AI applications. Using the Retrieval Augmented Generation (RAG) technique, a knowledge base searches your data to find the most relevant information and then uses it to answer natural language questions with context-specific responses.\n",
    "\n",
    "In this notebook, you will create an Amazon S3 vector store and integrate it with Amazon Bedrock Knowledge Bases. This powerful combination allows us to create a scalable and efficient system for storing, retrieving, and using embeddings in AI-powered applications.\n",
    "\n",
    "### What are Vector Stores?\n",
    "Vector stores are specialized databases designed to store and efficiently search through vector embeddings. These embeddings represent text or other data in a high-dimensional space where semantic similarity can be measured as distance between vectors.\n",
    "\n",
    "By combining Amazon S3 Vector Stores scalability and cost-effectiveness with Amazon Bedrock Knowledge Bases capabilities, we can build robust knowledge retrieval systems that can handle large volumes of data while providing accurate and context-aware responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d98661-6a84-4536-99c3-bdf2b0f731f6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To complete this notebook you should have:\n",
    "\n",
    "1. An AWS account with appropriate permissions\n",
    "2. A role with access to the following services: Amazon S3, AWS STS, and Amazon Bedrock\n",
    "3. Access to Amazon Bedrock models (specifically Anthropic Claude 3.5 Sonnet and Amazon Titan Text Embeddings V2)\n",
    "\n",
    "In the notebook, we will use a synthetic dataset of health reports to populate the Amazon Bedrock Knowledge Bases. You can use your own documents by:\n",
    "\n",
    "1. Upload your documents (data source) to an Amazon S3 bucket\n",
    "2. Note the Amazon S3 Bucket name and update the relevant sections in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22e67e-2b90-4c7a-8611-28f13e99133f",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's first install the required dependencies and initialize the boto3 clients we'll need throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d29953-a0a3-4736-b774-0053115785c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install or update boto3\n",
    "!pip install -qU boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09e250-bcaa-4515-b5fd-8475ac0bc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import pprint\n",
    "import requests\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from utils import generate_short_code, create_bedrock_execution_role, empty_and_delete_bucket, create_s3_bucket\n",
    "\n",
    "# Create boto3 session and get account information\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# Create s3vectors client\n",
    "s3vectors = boto3.client('s3vectors', region_name=region_name)\n",
    "\n",
    "# Create bedrock agent clients with extended timeouts for long-running operations\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "bedrock_agent_runtime_client = boto3_session.client(\"bedrock-agent-runtime\", config=bedrock_config)\n",
    "bedrock_agent_client = bedrock = boto3.client('bedrock-agent', region_name=region_name)\n",
    "\n",
    "# Generate unique identifier for resource names to avoid conflicts\n",
    "unique_id = generate_short_code()\n",
    "\n",
    "# Define resource names with unique identifiers\n",
    "bucket_name = f\"my-data-source-{unique_id}\"\n",
    "vector_store_name = f\"my-s3-vector-store-{unique_id}\"\n",
    "vector_index_name = f\"my-s3-vector-index-{unique_id}\"\n",
    "\n",
    "print(f\"Using unique identifier: {unique_id}\")\n",
    "print(f\"AWS Region: {region_name}\")\n",
    "print(f\"Account ID: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a38afe-34fe-46b1-a154-ba5cc6e30d03",
   "metadata": {},
   "source": [
    "## Create an Amazon S3 Vector store\n",
    "\n",
    "S3 Vector Store is a managed vector database solution directly integrated with Amazon S3.\n",
    "\n",
    "In this section, we'll create a vector bucket that will serve as our vector database. The vector bucket will store the vector embeddings of our documents, enabling semantic search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e62e60-0085-4b93-984c-9ca2fe9ea814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_bucket(vector_bucket_name):\n",
    "    \"\"\"Create an S3 Vector bucket and return its ARN\"\"\"\n",
    "    try:\n",
    "        # Create the vector bucket\n",
    "        s3vectors.create_vector_bucket(vectorBucketName=vector_bucket_name)\n",
    "        print(f\"✅ Vector bucket '{vector_bucket_name}' created successfully\")\n",
    "        \n",
    "        # Get the vector bucket details\n",
    "        response = s3vectors.get_vector_bucket(vectorBucketName=vector_bucket_name)\n",
    "        bucket_info = response.get(\"vectorBucket\", {})\n",
    "        vector_store_arn = bucket_info.get(\"vectorBucketArn\")\n",
    "        \n",
    "        if not vector_store_arn:\n",
    "            raise ValueError(\"Vector bucket ARN not found in response\")\n",
    "            \n",
    "        print(f\"Vector bucket ARN: {vector_store_arn}\")\n",
    "        return vector_store_arn\n",
    "    except ClientError as e:\n",
    "        error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "        error_message = e.response.get('Error', {}).get('Message', 'Unknown error')\n",
    "        print(f\"❌ Error creating vector bucket: {error_code} - {error_message}\")\n",
    "        raise\n",
    "\n",
    "# Create the vector bucket\n",
    "vector_store_arn = create_vector_bucket(vector_store_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3736f9a-1496-4569-bec1-2153eeab3e27",
   "metadata": {},
   "source": [
    "### Creating a Vector Index\n",
    "\n",
    "Now that we have created the vector store, we need to create a vector index. The vector index is where:\n",
    "\n",
    "1. Vector embeddings are stored and organized\n",
    "2. Similarity searches are performed\n",
    "3. Metadata about our documents is maintained\n",
    "\n",
    "We'll specify parameters like dimension (the size of our embedding vectors), distance metric (how similarity is calculated), and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837332f0-58e6-40f9-a67f-8cbf9d19c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensionality of our embedding vectors\n",
    "# This should match the output dimension of the embedding model we'll use (Titan Embed Text v2)\n",
    "vector_dimension = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a6565-5aa0-4d3b-bbab-1145053c7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_get_index_arn(s3vectors_client, vector_store_name, vector_index_name, vector_dimension):\n",
    "    \"\"\"\n",
    "    Create a vector index in the specified vector store and return its ARN\n",
    "    \n",
    "    Args:\n",
    "        s3vectors_client: Boto3 client for S3 Vectors\n",
    "        vector_store_name: Name of the vector store\n",
    "        vector_index_name: Name for the new index\n",
    "        vector_dimension: Dimension of the vectors (e.g., 1024 for Titan Embed)\n",
    "        \n",
    "    Returns:\n",
    "        str: ARN of the created index\n",
    "    \"\"\"\n",
    "    # Define index configuration\n",
    "    index_config = {\n",
    "        \"vectorBucketName\": vector_store_name,\n",
    "        \"indexName\": vector_index_name,\n",
    "        \"dimension\": vector_dimension,\n",
    "        \"distanceMetric\": \"cosine\",  # Using cosine similarity as our metric\n",
    "        \"dataType\": \"float32\",       # Standard for most embedding models\n",
    "        \"metadataConfiguration\": {\n",
    "            \"nonFilterableMetadataKeys\": [\"AMAZON_BEDROCK_TEXT\"]  # Text content won't be used for filtering\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create the index\n",
    "        s3vectors_client.create_index(**index_config)\n",
    "        print(f\"✅ Vector index '{vector_index_name}' created successfully\")\n",
    "\n",
    "        # Get the index ARN\n",
    "        response = s3vectors_client.list_indexes(vectorBucketName=vector_store_name)\n",
    "        index_arn = response.get(\"indexes\", [{}])[0].get(\"indexArn\")\n",
    "        \n",
    "        if not index_arn:\n",
    "            raise ValueError(\"Index ARN not found in response\")\n",
    "            \n",
    "        print(f\"Vector index ARN: {index_arn}\")\n",
    "        return index_arn\n",
    "\n",
    "    except ClientError as e:\n",
    "        error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "        error_message = e.response.get('Error', {}).get('Message', 'Unknown error')\n",
    "        print(f\"❌ Failed to create or retrieve index: {error_code} - {error_message}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e82d03-ff30-405d-ab18-299e37c316c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector index\n",
    "vector_index_arn = create_and_get_index_arn(\n",
    "    s3vectors,\n",
    "    vector_store_name,\n",
    "    vector_index_name,\n",
    "    vector_dimension)\n",
    "\n",
    "print(f\"\\nVector index created with ARN: {vector_index_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6793e-e05d-495c-ba7b-d0f2b2462aca",
   "metadata": {},
   "source": [
    "## Create an Amazon Bedrock Knowledge Base \n",
    "\n",
    "Before we create a Knowledge Base, we need to establish the appropriate IAM permissions. The Knowledge Base needs permissions to:\n",
    "\n",
    "1. Access and read documents from our S3 bucket\n",
    "2. Create and manage embeddings in our S3 Vector Store\n",
    "3. Interact with the Bedrock embedding models\n",
    "\n",
    "We'll create an IAM role with the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956969e-33f1-493e-8086-1a6e5e0f2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM role for Bedrock Knowledge Base\n",
    "create_role = create_bedrock_execution_role(unique_id, region_name, bucket_name, vector_store_name, vector_index_name, account_id)\n",
    "roleArn = create_role[\"Role\"][\"Arn\"]\n",
    "roleName = create_role[\"Role\"][\"RoleName\"]\n",
    "\n",
    "print(f\"Created IAM role: {roleName}\")\n",
    "print(f\"Role ARN: {roleArn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef848f0-e4d5-4aa4-a72b-068679715644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for IAM role propagation\n",
    "print(\"Waiting for IAM role propagation (60 seconds)...\")\n",
    "time.sleep(60)  # Wait for all policies and resources to be fully propagated\n",
    "\n",
    "# Define Knowledge Base name with unique identifier\n",
    "kb_name = f\"kb-s3-vectors-{unique_id}\"\n",
    "\n",
    "# Create the Knowledge Base\n",
    "create_kb_response = bedrock.create_knowledge_base(\n",
    "    name=kb_name,\n",
    "    description='Amazon Bedrock Knowledge Bases with S3 Vector Store',\n",
    "    roleArn=roleArn,\n",
    "    knowledgeBaseConfiguration={\n",
    "        'type': 'VECTOR',\n",
    "        'vectorKnowledgeBaseConfiguration': {\n",
    "            # Specify the embedding model to use\n",
    "            'embeddingModelArn': f'arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0',\n",
    "            'embeddingModelConfiguration': {\n",
    "                'bedrockEmbeddingModelConfiguration': {\n",
    "                    'dimensions': 1024,  # Should match the vector_dimension we defined earlier\n",
    "                    'embeddingDataType': 'FLOAT32'\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    storageConfiguration={\n",
    "        'type': 'S3_VECTORS',\n",
    "        's3VectorsConfiguration': {\n",
    "            'indexArn': f'arn:aws:s3vectors:{region_name}:{account_id}:bucket/{vector_store_name}/index/{vector_index_name}',\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "knowledge_base_id = create_kb_response[\"knowledgeBase\"][\"knowledgeBaseId\"]\n",
    "print(f\"Knowledge base ID: {knowledge_base_id}\")\n",
    "\n",
    "print(f\"\\nWaiting for knowledge base {knowledge_base_id} to finish creating...\")\n",
    "\n",
    "# Poll for KB creation status\n",
    "status = \"CREATING\"\n",
    "start_time = time.time()\n",
    "\n",
    "while status == \"CREATING\":\n",
    "    # Get current status\n",
    "    response = bedrock.get_knowledge_base(\n",
    "        knowledgeBaseId=knowledge_base_id\n",
    "    )\n",
    "    \n",
    "    status = response['knowledgeBase']['status']\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    \n",
    "    print(f\"Current status: {status} (elapsed time: {elapsed_time}s)\")\n",
    "    \n",
    "    if status == \"CREATING\":\n",
    "        print(\"Still creating, checking again in 30 seconds...\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"\\n✅ Knowledge base creation completed with status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e82f6-b95c-48a9-ac15-3fd9ab9a7078",
   "metadata": {},
   "source": [
    "## Create the Data Source\n",
    "\n",
    "Now we need to create a data source that the Knowledge Base will use. The data source contains the documents that will be processed, embedded, and indexed into our vector store. \n",
    "\n",
    "We'll follow these steps:\n",
    "1. Create an S3 bucket for our documents\n",
    "2. Download and upload synthetic medical transcript data\n",
    "3. Create a Bedrock data source pointing to our S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0656085-5b49-41c0-884b-2578d26d723a",
   "metadata": {},
   "source": [
    "### Creating an S3 Bucket for Data Source\n",
    "\n",
    "First, let's create the S3 Bucket that will hold our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7507a45-9a4e-4135-8907-6c1ac65f1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 bucket for our data source\n",
    "create_s3_bucket(bucket_name, region=region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003821f-195e-404c-8a9d-a6258046fad8",
   "metadata": {},
   "source": [
    "### Downloading and Preparing Sample Data\n",
    "\n",
    "For this example, we'll use a synthetic dataset of medical transcriptions. The dataset consists of PDF format transcriptions of simulated medical conversations and can be found in [this GitHub repository](https://github.com/nazmulkazi/dataset_automated_medical_transcription).\n",
    "\n",
    "The following code will download these files for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6037a89-698c-4efa-b0fb-284555ddb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder for downloaded files\n",
    "dataset_folder = \"source_transcripts\"\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "abs_path = os.path.abspath(dataset_folder)\n",
    "\n",
    "# GitHub API URL for the repository content\n",
    "repo_url = 'https://api.github.com/repos/nazmulkazi/dataset_automated_medical_transcription/contents/transcripts/source'\n",
    "headers = {'Accept': 'application/vnd.github.v3+json'}\n",
    "\n",
    "try:\n",
    "    # Get list of files from GitHub\n",
    "    response = requests.get(repo_url, headers=headers, timeout=20)\n",
    "    response.raise_for_status()  # Raise exception for non-200 responses\n",
    "    json_data = response.json()\n",
    "    \n",
    "    # Filter for files only\n",
    "    list_of_pdfs = [item for item in json_data if item['type'] == 'file']\n",
    "    query_parameters = {\"downloadformat\": \"pdf\"}\n",
    "    \n",
    "    # Display how many files will be downloaded\n",
    "    print(f\"Downloading {len(list_of_pdfs)} files from GitHub repository...\")\n",
    "    \n",
    "    # List of filenames for reference\n",
    "    transcripts = [pdf_dict['name'] for pdf_dict in list_of_pdfs]\n",
    "    \n",
    "    # Download counter for progress tracking\n",
    "    downloaded = 0\n",
    "    \n",
    "    # Download each file with progress indicator\n",
    "    for pdf_dict in list_of_pdfs:\n",
    "        pdf_name = pdf_dict['name']\n",
    "        file_url = pdf_dict['download_url']\n",
    "        \n",
    "        # Update progress\n",
    "        downloaded += 1\n",
    "        if downloaded % 5 == 0 or downloaded == len(list_of_pdfs):\n",
    "            print(f\"Progress: {downloaded}/{len(list_of_pdfs)} files\")\n",
    "        \n",
    "        # Download and save the file\n",
    "        r = requests.get(file_url, params=query_parameters, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        with open(os.path.join(dataset_folder, pdf_name), 'wb') as pdf_file:\n",
    "            pdf_file.write(r.content)\n",
    "    \n",
    "    print(f\"\\n✅ All {len(list_of_pdfs)} files have been downloaded to {abs_path}\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"❌ Error downloading files: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f6b6f-d9ad-46b4-a432-3ca84c316746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_folder_to_s3(folder_path, bucket_name, prefix=''):\n",
    "    \"\"\"\n",
    "    Upload all files from a folder to an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to the folder containing files to upload\n",
    "        bucket_name: Name of the S3 bucket\n",
    "        prefix: Prefix to add to the object names in S3 (optional)\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    upload_count = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    # Count total files first\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        total_files += len(files)\n",
    "    \n",
    "    print(f\"Uploading {total_files} files to S3 bucket '{bucket_name}'...\")\n",
    "    \n",
    "    # Upload files\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, folder_path)\n",
    "            s3_path = os.path.join(prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "            \n",
    "            try:\n",
    "                s3_client.upload_file(local_path, bucket_name, s3_path)\n",
    "                upload_count += 1\n",
    "                \n",
    "                # Show progress periodically\n",
    "                if upload_count % 5 == 0 or upload_count == total_files:\n",
    "                    print(f\"Progress: {upload_count}/{total_files} files uploaded\")\n",
    "                    \n",
    "            except ClientError as e:\n",
    "                print(f\"❌ Error uploading {local_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✅ Uploaded {upload_count} files to S3 bucket '{bucket_name}'\")\n",
    "\n",
    "# Upload our downloaded transcripts to S3\n",
    "upload_folder_to_s3(abs_path, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340dc5fe-9ff2-4f01-b6ab-268d9494f6a0",
   "metadata": {},
   "source": [
    "### Creating the Bedrock Data Source\n",
    "\n",
    "With our documents now stored in S3, we can create a data source in our Knowledge Base that points to this bucket. The data source configuration includes:\n",
    "\n",
    "1. The S3 bucket location\n",
    "2. Chunking strategy (how documents are split into manageable pieces)\n",
    "3. Data deletion policy\n",
    "\n",
    "The chunking strategy is particularly important as it affects how your documents are processed for retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80218a72-0a96-40cf-b9bc-7a61e08713dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data source\n",
    "data_source_response = bedrock.create_data_source(\n",
    "    knowledgeBaseId=knowledge_base_id,\n",
    "    name='AmazonS3DataSource',\n",
    "    description='Amazon S3 Data Source',\n",
    "    dataDeletionPolicy='DELETE',  # When data source is deleted, also delete the data\n",
    "    dataSourceConfiguration={\n",
    "        'type': 'S3',\n",
    "        's3Configuration': {\n",
    "            'bucketArn': f'arn:aws:s3:::{bucket_name}',\n",
    "        },\n",
    "    },\n",
    "    vectorIngestionConfiguration={\n",
    "        'chunkingConfiguration': {\n",
    "            'chunkingStrategy': 'FIXED_SIZE',  # Split documents into chunks of fixed size\n",
    "            'fixedSizeChunkingConfiguration': {\n",
    "                \"maxTokens\": 300,           # Maximum tokens per chunk\n",
    "                \"overlapPercentage\": 20     # Overlap between chunks to maintain context\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the data source ID\n",
    "datasource_id = data_source_response[\"dataSource\"][\"dataSourceId\"]\n",
    "print(f\"✅ Data source created with ID: {datasource_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52274ec7-7448-42a7-893a-a8deaf0b4c5d",
   "metadata": {},
   "source": [
    "## Sync the data source\n",
    "\n",
    "Now that we have created the data source, we need to start the ingestion job that will:\n",
    "\n",
    "1. Read documents from our S3 bucket\n",
    "2. Chunk them according to our configuration\n",
    "3. Generate embeddings for each chunk using the Titan Embed model\n",
    "4. Store both the embeddings and text in our S3 Vector Store\n",
    "\n",
    "This process may take several minutes depending on the size of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586835f-89b2-48d8-bd57-7023ac1d9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the ingestion job\n",
    "response_ingestion = bedrock.start_ingestion_job(\n",
    "    dataSourceId=datasource_id,\n",
    "    description='First sync',\n",
    "    knowledgeBaseId=knowledge_base_id\n",
    ")\n",
    "\n",
    "print(f\"Started ingestion job: {response_ingestion['ingestionJob']['ingestionJobId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d5355-ef87-4026-9320-9a62709d946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the ingestion job progress\n",
    "status = \"STARTING\"\n",
    "ingestion_job_id = response_ingestion['ingestionJob']['ingestionJobId']\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Monitoring ingestion job progress:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while status in [\"STARTING\", \"IN_PROGRESS\"]:\n",
    "    # Get current status\n",
    "    response = bedrock.get_ingestion_job(\n",
    "        dataSourceId=datasource_id,\n",
    "        knowledgeBaseId=knowledge_base_id,\n",
    "        ingestionJobId=ingestion_job_id\n",
    "    )\n",
    "    \n",
    "    status = response['ingestionJob']['status']\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    \n",
    "    # Get current statistics\n",
    "    stats = response['ingestionJob']['statistics']\n",
    "    \n",
    "    # Clear previous output and print updated status\n",
    "    print(f\"Status: {status} (elapsed time: {elapsed_time}s)\")\n",
    "    print(f\"Documents scanned: {stats['numberOfDocumentsScanned']}\")\n",
    "    print(f\"Documents indexed: {stats['numberOfNewDocumentsIndexed']}\")\n",
    "    print(f\"Documents failed: {stats['numberOfDocumentsFailed']}\")\n",
    "    \n",
    "    if status in [\"STARTING\", \"IN_PROGRESS\"]:\n",
    "        print(\"Checking again in 30 seconds...\\n\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"-\" * 50)\n",
    "if status == \"COMPLETE\":\n",
    "    print(f\"✅ Ingestion job completed successfully\")\n",
    "else:\n",
    "    print(f\"⚠️ Ingestion job ended with status: {status}\")\n",
    "    \n",
    "# Print final statistics\n",
    "print(f\"\\nFinal statistics:\")\n",
    "print(f\"  • Documents scanned: {stats['numberOfDocumentsScanned']}\")\n",
    "print(f\"  • Documents indexed: {stats['numberOfNewDocumentsIndexed']}\")\n",
    "print(f\"  • Documents failed: {stats['numberOfDocumentsFailed']}\")\n",
    "print(f\"  • Total elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbedc12-0ca8-4a3c-8b29-fa55be912b25",
   "metadata": {},
   "source": [
    "## Testing the Knowledge Base\n",
    "\n",
    "Now that our Knowledge Base is fully set up and populated with data, we can test it using two different approaches:\n",
    "\n",
    "1. **Retrieve and Generate API**: This is a single-step approach where Bedrock retrieves relevant information and generates a complete answer.\n",
    "2. **Retrieve API**: This gives you more control by returning only the retrieved chunks, allowing you to process them further in your application.\n",
    "\n",
    "### Testing the Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d683fa3-7bd9-4875-a8ca-96966ca654ee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Warning:</b> Make sure you have enabled Anthropic Claude 3 Sonnet access in the Amazon Bedrock Console (model access). If you don't have access to this model, you can substitute it with another model you have access to.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc1f98-d175-4e4b-a2ca-415b7d453c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our test query\n",
    "test_query = \"Who is Kitty?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Retrieving information and generating answer...\")\n",
    "\n",
    "try:\n",
    "    # Call the retrieve and generate API\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            \"text\": test_query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                'knowledgeBaseId': knowledge_base_id,\n",
    "                \"modelArn\": f\"arn:aws:bedrock:{region_name}::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "                \"retrievalConfiguration\": {\n",
    "                    \"vectorSearchConfiguration\": {\n",
    "                        \"numberOfResults\": 5,  # Number of chunks to retrieve\n",
    "                    } \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n----- Answer -----\")\n",
    "    print(response['output']['text'])\n",
    "    \n",
    "    # You can also access citation metadata\n",
    "    if 'citations' in response['output']:\n",
    "        print(\"\\n----- Citations -----\")\n",
    "        for citation in response['output']['citations']:\n",
    "            print(f\"- {citation['generatedResponsePart']['text']}\")\n",
    "            print(f\"  Source: {citation['retrievedReferences'][0]['location']}\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "    error_message = e.response.get('Error', {}).get('Message', 'Unknown error')\n",
    "    print(f\"❌ Error: {error_code} - {error_message}\")\n",
    "    \n",
    "    if \"ForbiddenException\" in str(e):\n",
    "        print(\"\\nYou might not have access to the Claude 3.5 Sonnet model.\")\n",
    "        print(\"Please verify your model access in the Amazon Bedrock Console or try a different model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af54940-a9e9-4baf-ae8f-9f081cca28f6",
   "metadata": {},
   "source": [
    "### Testing Knowledge Base with Retrieve API\n",
    "\n",
    "The retrieve API gives you more control by returning only the retrieved chunks. This is useful when you want to:\n",
    "\n",
    "- See exactly what content was retrieved\n",
    "- Process the retrieved information yourself\n",
    "- Implement your own custom RAG pipeline\n",
    "- Use a different model for generating the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c14f9-882b-4f57-9190-b40a1ae3235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our test query\n",
    "test_query = \"Who is Kitty?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Retrieving information...\")\n",
    "\n",
    "try:\n",
    "    # Call the retrieve API\n",
    "    response_ret = bedrock_agent_runtime_client.retrieve(\n",
    "        knowledgeBaseId=knowledge_base_id,\n",
    "        retrievalQuery={\n",
    "            \"text\": test_query\n",
    "        },\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": 5,  # Number of chunks to retrieve\n",
    "            } \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Helper function to print the retrieved results in a readable format\n",
    "    def response_print(retrieve_resp):\n",
    "        print(f\"\\nFound {len(retrieve_resp['retrievalResults'])} relevant chunks:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for num, chunk in enumerate(retrieve_resp['retrievalResults'], 1):\n",
    "            print(f\"\\n----- Chunk {num} -----\")\n",
    "            print(f\"Text: {chunk['content']['text']}\")\n",
    "            print(f\"\\nLocation: {chunk['location']}\")\n",
    "            print(f\"Score: {chunk['score']:.4f}\")  # Similarity score (higher is better)\n",
    "            print(f\"Metadata: {chunk['metadata']}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "    # Print the retrieved results\n",
    "    response_print(response_ret)\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "    error_message = e.response.get('Error', {}).get('Message', 'Unknown error')\n",
    "    print(f\"❌ Error: {error_code} - {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a11966-48af-44bc-b9af-bc4be36b25ac",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "To avoid ongoing charges, it's important to clean up the resources we've created. This includes:\n",
    "\n",
    "1. Deleting the Knowledge Base\n",
    "2. Deleting the S3 Vector Store\n",
    "3. Emptying and deleting the S3 Bucket\n",
    "4. Removing IAM roles and policies\n",
    "\n",
    "Run the cell below to clean up all resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10823dcb-af1e-4cc8-a95a-70e79be2e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting cleanup process...\\n\")\n",
    "\n",
    "# Delete Knowledge Base\n",
    "print(f\"Deleting Knowledge Base: {knowledge_base_id}\")\n",
    "try:\n",
    "    bedrock.delete_knowledge_base(knowledgeBaseId=knowledge_base_id)\n",
    "    print(\"✅ Knowledge Base deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting Knowledge Base: {str(e)}\")\n",
    "\n",
    "# Delete S3 Vector Store policy\n",
    "print(f\"\\nDeleting S3 Vector Store: {vector_store_name}\")\n",
    "try:\n",
    "    s3vectors.delete_vector_bucket_policy(vectorBucketName=vector_store_name)\n",
    "    print(\"✅ S3 Vector Store policy deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting Vector Store policy: {str(e)}\")\n",
    "\n",
    "# Empty and delete S3 Bucket\n",
    "print(f\"\\nEmptying and deleting S3 Bucket: {bucket_name}\")\n",
    "try:\n",
    "    empty_and_delete_bucket(bucket_name)\n",
    "    print(\"✅ S3 Bucket emptied and deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error emptying and deleting S3 Bucket: {str(e)}\")\n",
    "\n",
    "# Delete IAM Role and detach policies\n",
    "print(f\"\\nDeleting IAM Role: {roleName}\")\n",
    "try:\n",
    "    # List and detach all attached policies\n",
    "    attached_policies = iam_client.list_attached_role_policies(RoleName=roleName).get('AttachedPolicies', [])\n",
    "    for policy in attached_policies:\n",
    "        print(f\"Detaching policy: {policy['PolicyArn']}\")\n",
    "        iam_client.detach_role_policy(RoleName=roleName, PolicyArn=policy['PolicyArn'])\n",
    "    \n",
    "    # Delete the role\n",
    "    iam_client.delete_role(RoleName=roleName)\n",
    "    print(\"✅ IAM Role deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting IAM Role: {str(e)}\")\n",
    "\n",
    "print(\"\\n✅ All resources have been cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
